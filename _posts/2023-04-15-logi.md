---
title:  "로지스틱스 회귀 연습"
excerpt: "로지스틱스 회귀에 대해서 연습해보자"

categories:
  - Blog
tags:
  - [Blog, practice, Github, Git]

toc: true
toc_sticky: true
 
date: 2023-04-15
last_modified_at: 2023-04-16
---
## 로지스틱스 회귀 연습

[kaggle 링크](https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial#Logistic-Regression-Classifier-Tutorial-with-Python)를 토대로 연습해보자

이 연습에서는 호주에 비가 오는지 여부를 예측하기 위해 로지스틱회귀를 이용해 이진분류 모델을 훈련한다

---
## 목차
1. [로지스틱스 회귀 설명/분석 직관](#1.로지스틱스-회귀-설명/분석-직관)
2. [로지스틱스 회귀의 직관](#2.로지스틱스-회귀의-직관)
3. [로지스틱스 회귀의 가정](#3.로지스틱스-회귀의-가정)
4. [로지스티스 회귀의 종류](#4.로지스티스-회귀의-종류)
5. [라이브러리 불러오기](#5.라이브러리-불러오기)
6. [데이터셋 불러오기](#6.데이터셋-불러오기)
7. [데이터 분석 확인](#7.데이터-분석-확인)
8. [특징 벡터 대상 변수 선언](#8.특징-벡터-대상-변수-선언)
9. [데이터를 학습과 테스트 셋으로 분할](#9.데이터를-학습과-테스트-셋으로-분할)
10. [특성 처리](#10.특성-처리)
11. [특성 스케일링](#11.특성-스케일링)
12. [모델 학습](#12.모델-학습)
13. [결과 예측](#13.결과-예측)
14. [정확한 점수 예측](#14.정확한-점수-예측)
15. [혼돈 매트릭스](#15.혼돈-매트릭스)
16. [분류 지표](#16.분류-지표)
17. [임계값 조정](#17.임계값-조정)
18. [ROC-AUC](#18.ROC-AUC)

---
## 1.로지스틱스 회귀 설명/분석 직관

로지스틱 회귀는 데이터 분류에 있어서 가장 기본적인 알고리즘이다.

로지스틱 회귀는 지도학습 분류 알고리즘으로 관측값을 불연속적인 클래스 집합으로 예측하는데 사용된다.

따라서 결과 역시 불연속적이다.

로지스틱 회귀는 분류 문제를 가장 간단하게 해결하게 해주고 다양한 용도로 사용가능하다.

---
## 2.로지스틱스 회귀의 직관

통계학에서 로지스틱 회귀 모델은 분류목적으로 자주 사용된다.

일련의 관측값이 주어지면 로지스틱 회귀 모델을 통해 

관측값을 두 개 이상의 불연속적인 클래스로 분류하는데 도움이 된다.

따라서 대상은 본질적으로 이산형이다.

로지스틱 회귀 알고리즘은 다음과 같이 작동한다.

### 선형 방정식 구현

로지스틱 회귀 알고리즘은 독립 변수 또는 설명 변수로 선형 방정식을 구현하여 응답 값을 예측한다.

예를 들어 공부한 시간과 시험 합격 확률의 예를 살펴 보면, 공부 시간을 변수로 x1이라 하고

시험에 합격할 확률은 응답 또는 목표 변수이며 Z로 표시 된다.

하나의 설명 변수와 결과 변수가 있다면 선형 방정식은 다음과 같은 방정식으로 표현 된다.

```
z = b0 + b1x1
```
여기서 계수 b0, b1은 모델의 매개 변수이다.

변수가 여러개라면 더 확장이된다.

```
z = b0 + b1x1+ b2x2+, ... ,+ bnxn
```
이번에는 b0, b1, b2, .... bn이 모델의 매개 변수이다.

따라서 예측값은 위의 방정식으로 주어진다

### 시그모이드 함수

다음으로는 Z로 표시되는 예측 응답 값을 0과 1사이의 확률 값으로 변환 한다.

그런 다음 이 시그모이드 함수를 이용하여 모든 실제 값을 0과 1사이의 확률 값으로 적용시킨다.


머신 러닝에서 시그모이드 함수는 예측을 확률에 적용하는 데 사용된다.

시그모이드 함수는 S자의 곡선을 가지고 있어 시그모이드 곡선이라고 부르기도 한다.



시그모이드 함수는 로지스틱 함수의 특수한 경우로 다음과 같은 수학 공식으로 표현된다.

$$
Φ(z) = {\frac{1}{1 + e^{-z}}}
$$



그래프로 표현하면 다음과 같다

<img src ="https://miro.medium.com/v2/resize:fit:970/1*Xu7B5y9gp0iL5ooBj7LtWw.png">

### 의사 결정 경계

시그모이드 함수는 0과 1사이의 확률 값의 반환한다.

이 확률 값은 0또는 1인 불연속적인 클래스에 적용된다.

이 확률 값을 불연속적인 클래스(ex 예/아니오)에 적용하기 위해, 임계값을 선택한다.

이 임계값을 의사 결정 경계라고 하며 초과시 확률을 클래스 1에 적용하고  이하라면 클래스 0에 적용한다.


수학적으로는 다음과 같다

```
p≥ 0.5 -> 클래스 = 1

p < 0.5 -> 클래스 = 0

```

일반적으로 의사 결정 경계는 0.5로 설정되는데

따라서 확률값이 0.8일경우 0.5보다 크기때문에 1에 적용된다.

반대로 0.2인 경우 0.5보다 작기 때문에 에 적용된다.

그래프로 표현하면 다음과 같다.

<img src ="https://ml-cheatsheet.readthedocs.io/en/latest/_images/logistic_regression_sigmoid_w_threshold.png">

---
## 3.로지스틱스 회귀의 가정

---
## 4.로지스티스 회귀의 종류

---
## 5.라이브러리 불러오기

---
## 6.데이터셋 불러오기

---
## 7.데이터 분석 확인

---
## 8.특징 벡터 대상 변수 선언

---
## 9.데이터를 학습과 테스트 셋으로 분할

---
## 10.특성 처리

---
## 11.특성 스케일링

---
## 12.모델 학습

---
## 13.결과 예측

---
## 14.정확한 점수 예측

---
## 15.혼돈 매트릭스

---
## 16.분류 지표

---
## 17.임계값 조정

---
## 18.ROC-AUC


